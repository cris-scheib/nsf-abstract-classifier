{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the .xml files and create a .csv with only the needed information \n",
    "# Uses the AwardTitle and the AbstractNarration information to create the dataset\n",
    "\n",
    "import os, glob\n",
    "import csv\n",
    "\n",
    "# Get the data from the .xml tags\n",
    "def filterData(data, tag):\n",
    "    startTag = \"<%s>\" % (tag)\n",
    "    endTag = \"</%s>\" % (tag)\n",
    "    start = data.find(startTag) + len(startTag)\n",
    "    end = data.find(endTag)\n",
    "    return data[start:end]\n",
    "\n",
    "#Clean the data removing unnecessary tag\n",
    "def cleanData(data):\n",
    "    startTag = \"<![CDATA[\"\n",
    "    endTag = \"]]>\"\n",
    "    if startTag in data:\n",
    "        data = data[len(startTag):-len(endTag)]\n",
    "    return data\n",
    "\n",
    "csvfile = open('abstract_narrations.csv', 'w')\n",
    "writer = csv.writer(csvfile)\n",
    "writer.writerow(['title', 'narration'])\n",
    "\n",
    "emptyTags = [\"<AwardTitle/>\", \"<AbstractNarration/>\"]\n",
    "\n",
    "for filename in glob.glob('files/*.xml'):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r', encoding='UTF8') as f: # open in readonly mode\n",
    "        data = f.read()\n",
    "        if all(x not in data for x in emptyTags):\n",
    "            \n",
    "            title = filterData(data,\"AwardTitle\")\n",
    "            title = cleanData(title)\n",
    "            \n",
    "            narration = filterData(data,\"AbstractNarration\")\n",
    "            narration = cleanData(narration)\n",
    "            \n",
    "            writer.writerow([title, narration])\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Case the file abstract_narrations.csv already exists the code above is not necessary\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4bc48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('abstract_narrations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e943e9ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cristine.scheibler/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/cristine.scheibler/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99223155",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['narration'] = df['narration'].str.lower()\n",
    "df['narration'] = df['narration'].str.replace('&lt;br/&gt;', '')\n",
    "df['narration'] = df['narration'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd3c018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>narration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adaptive dynamic coordination of damping contr...</td>\n",
       "      <td>[in, the, last, decades, global, environmental...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAPID: On-mask Chemical Modulation of Respirat...</td>\n",
       "      <td>[non, technical, abstract, spread, of, infecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Collaborative Research: Biomass burning smoke ...</td>\n",
       "      <td>[microbes, are, found, in, all, environments, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SBIR Phase I:  AK-423: A broad-spectrum antivi...</td>\n",
       "      <td>[the, broader, impact, commercial, potential, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Nature of Coupled Heat and Mass Transport ...</td>\n",
       "      <td>[the, goal, of, this, project, is, to, underst...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Adaptive dynamic coordination of damping contr...   \n",
       "1  RAPID: On-mask Chemical Modulation of Respirat...   \n",
       "2  Collaborative Research: Biomass burning smoke ...   \n",
       "3  SBIR Phase I:  AK-423: A broad-spectrum antivi...   \n",
       "4  The Nature of Coupled Heat and Mass Transport ...   \n",
       "\n",
       "                                           narration  \n",
       "0  [in, the, last, decades, global, environmental...  \n",
       "1  [non, technical, abstract, spread, of, infecti...  \n",
       "2  [microbes, are, found, in, all, environments, ...  \n",
       "3  [the, broader, impact, commercial, potential, ...  \n",
       "4  [the, goal, of, this, project, is, to, underst...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c4db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = stopwords.words('english')\n",
    "df['narration'] = df['narration'].apply(lambda x: [word for word in x if not word in all_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72ba7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ef5768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d5aeec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "vocabulary_size = 25000\n",
    "\n",
    "def build_dataset(dataset):\n",
    "    narration = dataset['narration'].tolist()\n",
    "    narration = flatten(narration)\n",
    "\n",
    "    words = [['UNK', -1]]\n",
    "    words.extend(collections.Counter(narration).most_common(vocabulary_size - 1))\n",
    "\n",
    "    # Dicion√°rio\n",
    "    dictionary = dict()\n",
    "    for word, _ in words:\n",
    "        dictionary[word] = len(dictionary)\n",
    "\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "\n",
    "    for word in narration:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "\n",
    "    words[0][1] = unk_count\n",
    "\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    assert len(dictionary) == vocabulary_size\n",
    "\n",
    "    return data, words, dictionary, reverse_dictionary\n",
    "    \n",
    "def build_dataset_with_existing_dictionary(narration, dictionary):\n",
    "    data = list()\n",
    "    for word in narration:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # 'UNK'\n",
    "        data.append(index)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, words, dictionary, reverse_dictionary = build_dataset(train)\n",
    "test_data = {}\n",
    "\n",
    "for index, row in test.iterrows():\n",
    "    test_data[row['title']] = build_dataset_with_existing_dictionary(row['narration'],dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ef4c1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Palavras mais comuns (+UNK) [['UNK', 37878], ('project', 39960), ('research', 35125), ('using', 20186), ('students', 19098), ('data', 18760), ('support', 18453), ('award', 15211), ('broader', 15130), ('impacts', 15062)]\n",
      "\n",
      "Amostra:  CAREER: A Comprehensive and Lightweight Framework for Transcriptome Analysis [407, 2147, 1707, 193, 107, 125, 3552, 707, 1063, 68, 298, 414, 811, 702, 434, 193, 257, 1051, 7629, 298, 90, 707, 1063, 117, 67, 152, 289, 3099, 67, 326, 265, 193, 1113, 12143, 837, 23, 109, 215, 31, 191, 222, 438, 23, 109, 215, 159, 1131, 27, 134, 14774, 5, 816, 193, 1602, 415, 126, 171, 4214, 573, 1762, 908, 1140, 5864, 1453, 1456, 1250, 908, 2121, 1886, 908, 3457, 843, 472, 503, 2331, 146, 1092, 126, 51, 324, 5, 556, 521, 529, 179, 387, 3332, 13727, 1098, 865, 2358, 1949, 18259, 590, 335, 629, 11, 5, 816, 81, 1, 32, 11, 188, 676, 3546, 51, 1275, 707, 10413, 1063, 3, 1707, 5, 86, 817, 11, 5, 224, 1788, 797, 160, 1524, 1707, 12037, 4214, 6961, 11, 7691, 1453, 7304, 2121, 707, 1063, 51, 38, 451, 560, 193, 1530, 519, 1283, 257, 1332, 191, 149, 707, 1063, 68, 1519, 336, 208, 51, 216, 816, 348, 1, 105, 125, 11, 1797, 962, 676, 179, 159, 430, 785, 4570, 126, 2164, 875, 572, 1507, 73, 1565, 179, 159, 154, 342, 1, 91, 885, 161, 649, 50, 2079, 7550, 522, 105, 6721, 174, 384, 648, 11, 126, 2041, 51, 7369, 3674, 41, 4550, 1893, 76, 65, 3546, 2192, 51, 6964, 72, 568, 6696, 572, 327, 1112, 1063, 365, 41, 5864, 811, 568, 702, 434, 556, 1741, 171, 1076, 1573, 34, 3546, 51, 72, 66, 555, 109, 215, 83, 7691, 1611, 317, 0, 588, 96, 720, 85, 199, 38, 153, 865, 110, 543, 3546, 7629, 66, 51, 343, 11, 939, 28, 5, 133, 107, 13612, 540, 2749, 1867, 3546, 3257, 535, 937, 459, 2901, 1104, 463, 45, 712, 980, 8118, 998, 68, 65, 1264, 573, 923, 404, 823, 3257, 160, 223, 843, 1257, 1516, 10413, 591, 2192, 26, 107, 28, 11, 159, 128, 1516, 3096, 10413, 1867, 4858, 275, 3096, 17911, 257, 95, 1076, 1949, 6665, 587, 1555, 86, 324, 2121, 24, 1705, 355, 191, 936, 4951, 5, 107, 86, 2812, 83, 95, 1200, 191, 16201, 24219, 0, 707, 1063, 1629, 160, 1127, 2851, 463, 155, 605, 45, 1033, 463, 58, 208, 99, 3107, 2192, 597, 1063, 412, 444, 5, 109, 104, 18335, 13161, 86, 2287, 24, 219, 276, 362, 216]\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(test_data))\n",
    "print('\\nPalavras mais comuns (+UNK)', words[:10])\n",
    "print('\\nAmostra: ', sample, test_data[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7fa1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
